context_length: 96
dataloader_num_workers: 1
gradient_accumulation_steps: 1
learning_rate: 0.001
log_steps: 500
lr_scheduler_type: polynomial
max_missing_prop: 0.9
max_steps: 200000
min_past: 96
model_id: amazon/chronos-t5-mini
model_type: seq2seq
n_tokens: 4096
num_samples: 20
optim: adamw_torch_fused
output_dir: ./finetuned_model/
per_device_train_batch_size: 32
prediction_length: 12
probability:
- 1.0
random_init: false
save_steps: 100000
shuffle_buffer_length: 100000
tf32: true
tie_embeddings: true
tokenizer_class: MeanScaleUniformBins
tokenizer_kwargs:
  high_limit: 15.0
  low_limit: -15.0
torch_compile: true
training_data_paths:
- /home/ubuntu/tsmixup-data.arrow
- /home/ubuntu/kernelsynth-data.arrow
use_eos_token: true
warmup_ratio: 0.0
