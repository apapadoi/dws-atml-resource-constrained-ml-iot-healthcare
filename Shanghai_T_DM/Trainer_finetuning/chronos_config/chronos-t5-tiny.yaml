context_length: 192
dataloader_num_workers: 1
gradient_accumulation_steps: 1
learning_rate: 0.001
log_steps: 500
lr_scheduler_type: cosine
max_missing_prop: 0.9
max_steps: 200000
min_past: 192
model_id: amazon/chronos-t5-tiny
model_type: seq2seq
n_tokens: 4096
num_samples: 20
optim: adamw_torch_fused
output_dir: ./finetuned_model/
per_device_train_batch_size: 32
prediction_length: 24
probability:
- 1.0
random_init: false
save_steps: 100000
shuffle_buffer_length: 100000
tf32: true
tie_embeddings: true
tokenizer_class: MeanScaleUniformBins
tokenizer_kwargs:
  high_limit: 15.0
  low_limit: -15.0
torch_compile: true
training_data_paths:
- shanghai_train_dataset.arrow
use_eos_token: true
warmup_ratio: 0.0
