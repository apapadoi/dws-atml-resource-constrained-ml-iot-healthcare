{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a147a16-af58-450d-827c-abbac18c7bfe",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "980a7624-8f63-42bb-a30b-926c4b88168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GPTQConfig, AutoConfig, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "from chronos import ChronosPipeline, ChronosModel, ChronosConfig\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac94409-0076-42c7-b00e-dfc54edf44c3",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "003b7583-ceb2-4b41-a7fa-d9b9a57d30d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, context_length, prediction_length):\n",
    "        self.data = data\n",
    "        self.context_length = context_length\n",
    "        self.prediction_length = prediction_length\n",
    "\n",
    "    def __len__(self):\n",
    "        # Each dataframe contributes (len(df) - context_length - prediction_length + 1) samples\n",
    "        return sum(len(df) - self.context_length - self.prediction_length + 1 for df in self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Find the appropriate dataframe and index within that dataframe\n",
    "        cumulative_length = 0\n",
    "        for df in self.data:\n",
    "            current_length = len(df) - self.context_length - self.prediction_length + 1\n",
    "            if idx < cumulative_length + current_length:\n",
    "                local_idx = idx - cumulative_length\n",
    "                context = df['CGM'].values[local_idx:local_idx + self.context_length]\n",
    "                target = df['CGM'].values[local_idx + self.context_length:local_idx + self.context_length + self.prediction_length]\n",
    "                return torch.tensor(context), torch.tensor(target)\n",
    "            cumulative_length += current_length\n",
    "        raise IndexError(\"Index out of range in dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "527c0052-b3dd-4a6a-8455-5e42d3606d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(dataset_name, data, split_ratios,context_window,prediction_window):\n",
    "\n",
    "    # Load the datasets\n",
    "    print(f\"Loading {dataset_name} datasets...\")\n",
    "\n",
    "    train_data, val_data, test_data = [], [], []\n",
    "    train_ratio,val_ratio,test_ratio = split_ratios\n",
    "\n",
    "    assert(train_ratio+val_ratio+test_ratio==1)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        num_rows = len(data[i])\n",
    "        sample_size = context_window + prediction_window\n",
    "        if num_rows>sample_size:\n",
    "            if num_rows < 3*sample_size + 1:\n",
    "                train_data.append(data[i])\n",
    "            else:\n",
    "                available_samples = (num_rows - 3*sample_size - 1)\n",
    "                training_samples = int(available_samples * train_ratio)\n",
    "                validation_samples = int(available_samples * val_ratio)\n",
    "                testing_samples = int(available_samples * test_ratio)\n",
    "\n",
    "                if validation_samples == 0:\n",
    "                    training_samples -= 1\n",
    "                    validation_samples += 1\n",
    "                if testing_samples == 0:\n",
    "                    training_samples -= 1\n",
    "                    testing_samples += 1\n",
    "\n",
    "                train_data.append(data[i].iloc[:training_samples + sample_size + 1])\n",
    "                val_data.append(data[i].iloc[training_samples + sample_size + 1:training_samples + validation_samples + 2*sample_size + 2])\n",
    "                test_data.append(data[i].iloc[training_samples + validation_samples + 2*sample_size + 2:])\n",
    "\n",
    "    # TimeSeries Dataloaders\n",
    "    train_dataset = TimeSeriesDataset(train_data, context_window, prediction_window)\n",
    "    val_dataset = TimeSeriesDataset(val_data, context_window, prediction_window)\n",
    "    test_dataset = TimeSeriesDataset(test_data, context_window, prediction_window)\n",
    "\n",
    "    print(f\"Number of training examples: {len(train_dataset)}\")\n",
    "    print(f\"Number of validation examples: {len(val_dataset)}\")\n",
    "    print(f\"Number of test examples: {len(test_dataset)}\")\n",
    "\n",
    "    return train_dataset,val_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0d49b0b-f885-4c92-8e84-5e7d2ad8a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load model with quantization\n",
    "def load_model(model_name, quantization_bits=None):\n",
    "    if quantization_bits == 8:\n",
    "        bnb_config_8bit = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_threshold=0.0\n",
    "        )\n",
    "        pipeline = ChronosPipeline.from_pretrained(model_name,low_cpu_mem_usage=True,device_map=\"cuda:0\",quantization_config=bnb_config_8bit)\n",
    "    elif quantization_bits == 4:\n",
    "        bnb_config_4bit = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_storage=torch.bfloat16\n",
    "        )\n",
    "        pipeline = ChronosPipeline.from_pretrained(model_name,low_cpu_mem_usage=True,device_map=\"cuda:0\",quantization_config=bnb_config_4bit)\n",
    "    else:\n",
    "        pipeline = ChronosPipeline.from_pretrained(model_name,low_cpu_mem_usage=True,device_map=\"cuda:0\")\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def calculate_rmse(predictions, targets):\n",
    "    return np.sqrt(mean_squared_error(predictions, targets))\n",
    "\n",
    "def mean_absolute_percentage_error(predictions,targets):\n",
    "    return np.mean(np.abs((targets - predictions) / targets)) * 100\n",
    "\n",
    "# Function to print the dtype, count, and model size of parameters\n",
    "def print_model_dtype_and_size(model):\n",
    "    param_dtypes = {}\n",
    "    dtype_size = {\n",
    "        torch.float32: 4,\n",
    "        torch.float16: 2,\n",
    "        torch.bfloat16: 2,\n",
    "        torch.int8: 1,\n",
    "        torch.uint8: 1\n",
    "    }\n",
    "    total_size = 0\n",
    "\n",
    "    total_parameters = 0\n",
    "    for param in model.parameters():\n",
    "        dtype = param.dtype\n",
    "        count_param = param.numel()\n",
    "        total_parameters+=count_param\n",
    "        if dtype not in param_dtypes:\n",
    "            param_dtypes[dtype] = 0\n",
    "        param_dtypes[dtype] += count_param\n",
    "\n",
    "    for dtype, count in param_dtypes.items():\n",
    "        size = count * dtype_size[dtype]\n",
    "        total_size += size\n",
    "        print(f\"Dtype: {dtype}, Count: {count}, Size: {size / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "    print(f\"Total parameters: {total_parameters}\")\n",
    "    print(f\"Total model size: {total_size / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "# Function to perform inference and measure time with tqdm progress bar\n",
    "def test_model(pipeline, dataloader, setting_name, batch_size, num_samples=10):\n",
    "\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    inference_times = []\n",
    "    all_inputs = []\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Testing\"):\n",
    "\n",
    "            #all_inputs.append(inputs.cpu().numpy())\n",
    "            #all_outputs.append(targets.cpu().numpy())\n",
    "            start_time = time.time()\n",
    "            outputs = pipeline.predict(inputs,prediction_length=prediction_window,num_samples=num_samples)\n",
    "            end_time = time.time()\n",
    "\n",
    "            # Calculate the low, median, and high quantiles across the second dimension (dimension with size 3)\n",
    "            median_quantile = torch.quantile(outputs, q=0.5, dim=1)\n",
    "\n",
    "            batch_time = (end_time - start_time)/num_samples\n",
    "            inference_times.append((batch_time/batch_size))\n",
    "\n",
    "            all_predictions.append(median_quantile.numpy())\n",
    "            all_targets.append(targets.numpy())\n",
    "\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    #all_inputs = np.concatenate(all_inputs, axis=0)\n",
    "    #all_outputs = np.concatenate(all_outputs, axis=0)\n",
    "\n",
    "    mape = mean_absolute_percentage_error(all_predictions.reshape(all_targets.shape[0],all_targets.shape[1]), all_targets)\n",
    "    rmse = calculate_rmse(all_predictions.reshape(all_targets.shape[0],all_targets.shape[1]), all_targets)\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "\n",
    "    print(f\"{setting_name}. RMSE: {rmse:.2f}, MAPE: {mape:.2f}, Avg Inference Time: {(avg_inference_time*1e3):.3f}\")\n",
    "\n",
    "    return all_inputs, all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0163b757-33dc-4384-aedf-a9e51f997c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_samples_per_hour = 4\n",
    "context_window = 24 * num_samples_per_hour  # 48 hours of data, 4 points per hour (every 15 minutes)\n",
    "prediction_window = 3 * num_samples_per_hour  # 6 hours of data, 4 points per hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e9e474-76e6-4a88-93fc-94bf61cda318",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b15ec844-ffa8-4ee0-bf56-44f171a8ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset options 'ShanghaiT1DM','ShanghaiT2DM','Patient0', 'Dinamo_Shanghai_T1DM'\n",
    "\n",
    "##### Option 1\n",
    "#dataset = 'Dinamo_Shanghai_T1DM'\n",
    "#base_model = \"amazon/chronos-t5-tiny\"\n",
    "#finetuned_model = 'moschouChry/finetuned-chronos-tiny-type-1'\n",
    "\n",
    "##### Option 2\n",
    "#dataset = 'Dinamo_Shanghai_T1DM'\n",
    "#base_model = \"amazon/chronos-t5-small\"\n",
    "#finetuned_model = 'moschouChry/finetuned-chronos-small-type-1'\n",
    "\n",
    "##### Option 3\n",
    "#dataset = 'ShanghaiT2DM'\n",
    "#base_model = \"amazon/chronos-t5-tiny\"\n",
    "#finetuned_model = 'moschouChry/finetuned-chronos-tiny-type-2'\n",
    "\n",
    "##### Option 4\n",
    "#dataset = 'ShanghaiT2DM'\n",
    "#base_model = \"amazon/chronos-t5-small\"\n",
    "#finetuned_model = 'moschouChry/finetuned-chronos-small-type-2'\n",
    "\n",
    "##### Option 5\n",
    "#dataset = 'Patient0'\n",
    "#base_model = \"amazon/chronos-t5-tiny\"\n",
    "#finetuned_model = 'moschouChry/finetuned-chronos-tiny-patient-0'\n",
    "\n",
    "##### Option 6\n",
    "#dataset = 'Patient0'\n",
    "#base_model = \"amazon/chronos-t5-tiny\"\n",
    "#finetuned_model = 'moschouChry/finetuned-chronos-tiny-patient-0'\n",
    "\n",
    "##### Option 7\n",
    "dataset = 'Patient0'\n",
    "base_model = \"amazon/chronos-t5-tiny\"\n",
    "finetuned_model =  'moschouChry/finetuned-chronos-tiny-patient-0'\n",
    "finetuned_model_1 = 'moschouChry/finetuned-chronos-tiny-type-1_no_lora'\n",
    "finetuned_model_2 = 'moschouChry/finetuned-chronos-tiny-type-1_lora_option_1'\n",
    "finetuned_model_3 = 'moschouChry/finetuned-chronos-tiny-type-1_lora_option_2'\n",
    "\n",
    "# Load the dictionary from the file\n",
    "with open('dataset_dictionary.pkl', 'rb') as file:\n",
    "    loaded_data_dict = pickle.load(file)\n",
    "\n",
    "data = loaded_data_dict[dataset]\n",
    "dataset_name = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdca2a2e-94fa-417f-81cc-9402ea44f7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ShanghaiT2DM datasets...\n",
      "Number of training examples: 69537\n",
      "Number of validation examples: 3982\n",
      "Number of test examples: 3920\n"
     ]
    }
   ],
   "source": [
    "train_ratio=0.9\n",
    "val_ratio=0.05\n",
    "test_ratio=0.05\n",
    "\n",
    "split_ratios = [train_ratio,val_ratio,test_ratio]\n",
    "train_dataset,val_dataset,test_dataset = create_dataloaders(dataset_name,data,split_ratios,context_window,prediction_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24eb2d9c-88cd-4ede-80f2-a3db9a1e24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b99e5-42e2-4487-823b-d33f11aed90a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#settings = {\n",
    "#    \"Chronos_Original\": [base_model, None],\n",
    "#    \"Chronos_finetuned Type 2\": [finetuned_model, None],\n",
    "#    \"Chronos_finetuned Type 2 8 bit\": [finetuned_model,8],\n",
    "#    \"Chronos_finetuned Type 2 4 bit\": [finetuned_model,4]\n",
    "#}\n",
    "\n",
    "settings = {\n",
    "    \"Chronos_Original\": [base_model, None],\n",
    "    \"Chronos_finetuned Type 1\": [finetuned_model, None],\n",
    "    \"Chronos_finetuned Patient 0 (No Lora)\": [finetuned_model_1,None],\n",
    "    \"Chronos_finetuned Patient 0 (Lora 1)\": [finetuned_model_3,None],\n",
    "    \"Chronos_finetuned Patient 0 (Lora 2)\": [finetuned_model_2,None],\n",
    "}\n",
    "\n",
    "for setting in settings.items():\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    setting_name = setting[0]\n",
    "    model_name, quantization_bits = setting[1]\n",
    "\n",
    "    print(setting_name)\n",
    "    NUM_SAMPLES = 20\n",
    "    if setting_name==\"Chronos_finetuned Patient 0 (No Lora)\":\n",
    "        NUM_SAMPLES = 50 #default 20\n",
    "    if setting_name==\"Chronos_finetuned Patient 0 (Lora 1)\":\n",
    "        NUM_SAMPLES = 50 #default 20\n",
    "    if setting_name==\"Chronos_finetuned Patient 0 (Lora 2)\":\n",
    "        NUM_SAMPLES = 50 #default 20\n",
    "    pipeline = load_model(model_name, quantization_bits)\n",
    "    print_model_dtype_and_size(pipeline.model)\n",
    "\n",
    "    test_model(pipeline, test_dataloader, setting_name, batch_size, NUM_SAMPLES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dws_atml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
