{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess blood glucose datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics(dataframes):\n",
    "    min_values = []\n",
    "    mean_values = []\n",
    "    max_values = []\n",
    "    lengths = []\n",
    "    \n",
    "    for df in dataframes:\n",
    "        min_values.append(df['CGM'].min())\n",
    "        mean_values.append(df['CGM'].mean())\n",
    "        max_values.append(df['CGM'].max())\n",
    "        lengths.append(len(df))\n",
    "    \n",
    "    # Calculating the average statistics\n",
    "    avg_min_value = sum(min_values) / len(min_values)\n",
    "    avg_mean_value = sum(mean_values) / len(mean_values)\n",
    "    avg_max_value = sum(max_values) / len(max_values)\n",
    "    \n",
    "    # Calculating overall statistics\n",
    "    all_values = pd.concat([df['CGM'] for df in dataframes])\n",
    "    overall_avg_value = all_values.mean()\n",
    "    overall_min_value = all_values.min()\n",
    "    overall_max_value = all_values.max()\n",
    "    \n",
    "    # Calculating the average length of the timeseries\n",
    "    avg_length = sum(lengths) / len(lengths)\n",
    "    \n",
    "    # Printing the statistics\n",
    "    print(f\"Average minimum value of the timeseries: {avg_min_value}\")\n",
    "    print(f\"Average mean value of the timeseries: {avg_mean_value}\")\n",
    "    print(f\"Average maximum value of the timeseries: {avg_max_value}\")\n",
    "    print(f\"Average value from all the timeseries: {overall_avg_value}\")\n",
    "    print(f\"Minimum value from all the timeseries: {overall_min_value}\")\n",
    "    print(f\"Maximum value from all the timeseries: {overall_max_value}\")\n",
    "    print(f\"Average length of the timeseries: {avg_length}\")\n",
    "    print(f\"Number of timeseries: {len(dataframes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_15_min_intervals(dataframes):\n",
    "    \"\"\"\n",
    "    Checks whether the 'Date' column in each DataFrame in the list has timestamps exactly 15 minutes apart.\n",
    "\n",
    "    Parameters:\n",
    "    dataframes (list of pd.DataFrame): List of DataFrames to check.\n",
    "\n",
    "    Returns:\n",
    "    results (list of bool): List of booleans indicating if each DataFrame has 15-minute intervals.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for df in dataframes:\n",
    "        df = df.sort_values(by='Date').reset_index(drop=True)\n",
    "        # Calculate the difference in minutes between consecutive timestamps\n",
    "        time_diffs = df['Date'].diff().dropna().dt.total_seconds() / 60\n",
    "        # Check if all differences are exactly 15 minutes \n",
    "        if not (time_diffs == 15).all():\n",
    "            print(f'Found intervals {time_diffs[(time_diffs != 15)].values} minutes apart!')\n",
    "        results.append((time_diffs == 15).all())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_5_min_intervals(dataframes):\n",
    "    \"\"\"\n",
    "    Checks whether the 'Date' column in each DataFrame in the list has timestamps exactly 15 minutes apart.\n",
    "\n",
    "    Parameters:\n",
    "    dataframes (list of pd.DataFrame): List of DataFrames to check.\n",
    "\n",
    "    Returns:\n",
    "    results (list of bool): List of booleans indicating if each DataFrame has 15-minute intervals.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for df in dataframes:\n",
    "        df = df.sort_values(by='Date').reset_index(drop=True)\n",
    "        # Calculate the difference in minutes between consecutive timestamps\n",
    "        time_diffs = df['Date'].diff().dropna().dt.total_seconds() / 60\n",
    "        # Check if all differences are exactly 15 minutes \n",
    "        if not (time_diffs == 5).all():\n",
    "            print(f'Found intervals {time_diffs[(time_diffs != 5)].values} minutes apart!')\n",
    "        results.append((time_diffs == 5).all())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_fix_15_min_intervals(dataframes,sample_size):\n",
    "    \"\"\"\n",
    "    Checks whether the 'Date' column in each DataFrame in the list has timestamps exactly 15 minutes apart.\n",
    "    If not, it corrects the intervals by adding missing timestamps and interpolating the 'CGM' values.\n",
    "\n",
    "    Parameters:\n",
    "    dataframes (list of pd.DataFrame): List of DataFrames to check and fix.\n",
    "\n",
    "    Returns:\n",
    "    results (list of pd.DataFrame): List of corrected DataFrames.\n",
    "    \"\"\"\n",
    "    corrected_dataframes = []\n",
    "    for df in dataframes:\n",
    "        df = df.sort_values(by='Date').reset_index(drop=True)\n",
    "        # Calculate the difference in minutes between consecutive timestamps\n",
    "        time_diffs = df['Date'].diff().dropna().dt.total_seconds() / 60\n",
    "\n",
    "        while not (time_diffs == 15).all():\n",
    "            # Find the indices where the difference is not 15 minutes\n",
    "            incorrect_intervals = time_diffs[time_diffs != 15].index\n",
    "            idx = incorrect_intervals[0]\n",
    "\n",
    "            if time_diffs[idx] != 30:\n",
    "                df1 = df.iloc[:idx - 1]\n",
    "                corrected_dataframes.append(df1)\n",
    "                df = df.iloc[idx:].reset_index(drop=True)\n",
    "\n",
    "            else:\n",
    "                # Insert missing rows to correct the intervals\n",
    "                prev_row = df.iloc[idx - 1]\n",
    "                curr_row = df.iloc[idx]\n",
    "\n",
    "                missing_time = prev_row['Date'] + pd.Timedelta(minutes=15)\n",
    "                missing_value = (prev_row['CGM'] + curr_row['CGM']) / 2  # Linear interpolation\n",
    "\n",
    "                # Create a new row with the interpolated value\n",
    "                new_row = pd.DataFrame({'Date': [missing_time], 'CGM': [missing_value]})\n",
    "\n",
    "                # Insert the new row into the DataFrame\n",
    "                df = pd.concat([df.iloc[:idx], new_row, df.iloc[idx:]]).reset_index(drop=True)\n",
    "\n",
    "                # Recalculate the time differences\n",
    "            time_diffs = df['Date'].diff().dropna().dt.total_seconds() / 60\n",
    "\n",
    "        corrected_dataframes.append(df)\n",
    "\n",
    "    corrected_dataframes = [df for df in corrected_dataframes if len(df) >= sample_size]\n",
    "\n",
    "    return corrected_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_fix_5_min_intervals(dataframes,sample_size):\n",
    "    \"\"\"\n",
    "    Checks whether the 'Date' column in each DataFrame in the list has timestamps exactly 15 minutes apart.\n",
    "    If not, it corrects the intervals by adding missing timestamps and interpolating the 'CGM' values.\n",
    "\n",
    "    Parameters:\n",
    "    dataframes (list of pd.DataFrame): List of DataFrames to check and fix.\n",
    "\n",
    "    Returns:\n",
    "    results (list of pd.DataFrame): List of corrected DataFrames.\n",
    "    \"\"\"\n",
    "    corrected_dataframes = []\n",
    "    count = 0\n",
    "    for df in dataframes:\n",
    "        df = df.sort_values(by='Date').reset_index(drop=True)\n",
    "        # Calculate the difference in minutes between consecutive timestamps\n",
    "        time_diffs = df['Date'].diff().dropna().dt.total_seconds() / 60\n",
    "        while not (time_diffs == 5).all():\n",
    "            # Find the indices where the difference is not 15 minutes\n",
    "            incorrect_intervals = time_diffs[time_diffs != 5].index\n",
    "            idx = incorrect_intervals[0]\n",
    "\n",
    "            if time_diffs[idx] >= 10:\n",
    "                df1 = df.iloc[:idx - 1]\n",
    "                corrected_dataframes.append(df1)\n",
    "                df = df.iloc[idx:].reset_index(drop=True)\n",
    "            elif (time_diffs[idx] < 10) & (time_diffs[idx] > 5):\n",
    "                # Insert missing rows to correct the intervals\n",
    "                prev_row = df.iloc[idx - 1]\n",
    "                curr_row = df.iloc[idx]\n",
    "\n",
    "                missing_time = prev_row['Date'] + pd.Timedelta(minutes=5)\n",
    "                missing_value = (prev_row['CGM'] + curr_row['CGM']) / 2  # Linear interpolation\n",
    "\n",
    "                # Create a new row with the interpolated value\n",
    "                new_row = pd.DataFrame({'Date': [missing_time], 'CGM': [missing_value]})\n",
    "\n",
    "                # Insert the new row into the DataFrame\n",
    "                df = pd.concat([df.iloc[:idx], new_row, df.iloc[idx:]]).reset_index(drop=True)\n",
    "            else:\n",
    "                df = df.drop(idx).reset_index(drop=True)\n",
    "                # Recalculate the time differences\n",
    "            time_diffs = df['Date'].diff().dropna().dt.total_seconds() / 60\n",
    "\n",
    "        corrected_dataframes.append(df)\n",
    "\n",
    "    corrected_dataframes = [df for df in corrected_dataframes if len(df) >= sample_size]\n",
    "\n",
    "    return corrected_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define context window and prediction window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each timeseries has to have at least 108 points\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_HOURS = 24\n",
    "PREDICTION_HOURS = 3\n",
    "\n",
    "samples_per_hour = 4\n",
    "prediction_length = PREDICTION_HOURS * samples_per_hour\n",
    "context_length = CONTEXT_HOURS * samples_per_hour\n",
    "\n",
    "sample_size = prediction_length + context_length\n",
    "print(f\"Each timeseries has to have at least {sample_size} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.9\n",
    "VAL_RATIO = 0.05\n",
    "TEST_RATIO = 0.05\n",
    "\n",
    "split_ratios = [TRAIN_RATIO, VAL_RATIO, TEST_RATIO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dictionary = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shanghai T1DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"../../datasets/Shanghai_T1DM\"\n",
    "\n",
    "dataframes = []\n",
    "file_names = []\n",
    "\n",
    "excel_files = glob.glob(os.path.join(file_path, \"*.xls\")) + glob.glob(os.path.join(file_path, \"*.xlsx\"))\n",
    "\n",
    "for file in excel_files:\n",
    "    df = pd.read_excel(file)\n",
    "    if 'CGM (mg / dl)' in df.columns:\n",
    "        df = df[['Date', 'CGM (mg / dl)']]\n",
    "    else:\n",
    "        df = df[['Date', 'CGM ']]\n",
    "\n",
    "    if df.isna().any().any():\n",
    "        print(f\"DataFrame {file} has columns with NaN values:\")\n",
    "        print(df.isna().sum())\n",
    "        continue\n",
    "\n",
    "    dataframes.append(df.sort_values(by='Date').rename(columns={\n",
    "        'CGM (mg / dl)': 'CGM',\n",
    "        'CGM ': 'CGM'\n",
    "    }))\n",
    "\n",
    "    file_names.append(file.split('.')[-2].split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found intervals [30.] minutes apart!\n"
     ]
    }
   ],
   "source": [
    "check = check_15_min_intervals(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shanghai T1DM is in the appropriate time format\n"
     ]
    }
   ],
   "source": [
    "corrected_dataframes_shanghaiT1DM = check_and_fix_15_min_intervals(dataframes,sample_size)\n",
    "\n",
    "check = check_15_min_intervals(corrected_dataframes_shanghaiT1DM)\n",
    "if check == [True for i in range(len(check))]:\n",
    "    print('Shanghai T1DM is in the appropriate time format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minimum value of the timeseries: 47.1375\n",
      "Average mean value of the timeseries: 163.6159111235018\n",
      "Average maximum value of the timeseries: 365.40000000000003\n",
      "Average value from all the timeseries: 164.74587155963303\n",
      "Minimum value from all the timeseries: 39.6\n",
      "Maximum value from all the timeseries: 475.2\n",
      "Average length of the timeseries: 981.0\n",
      "Number of timeseries: 16\n"
     ]
    }
   ],
   "source": [
    "print_statistics(corrected_dataframes_shanghaiT1DM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dictionary['ShanghaiT1DM'] = corrected_dataframes_shanghaiT1DM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shanghai T2DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame ../../datasets/Shanghai_T2DM\\2029_0_20210526.xls has columns with NaN values:\n",
      "Date              0\n",
      "CGM (mg / dl)    13\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "file_path = f\"../../datasets/Shanghai_T2DM\"\n",
    "\n",
    "dataframes = []\n",
    "file_names = []\n",
    "\n",
    "excel_files = glob.glob(os.path.join(file_path, \"*.xls\")) + glob.glob(os.path.join(file_path, \"*.xlsx\"))\n",
    "\n",
    "for file in excel_files:\n",
    "    df = pd.read_excel(file)\n",
    "    if 'CGM (mg / dl)' in df.columns:\n",
    "        df = df[['Date', 'CGM (mg / dl)']]\n",
    "    else:\n",
    "        df = df[['Date', 'CGM ']]\n",
    "\n",
    "    if df.isna().any().any():\n",
    "        print(f\"DataFrame {file} has columns with NaN values:\")\n",
    "        print(df.isna().sum())\n",
    "        continue\n",
    "\n",
    "    dataframes.append(df.sort_values(by='Date').rename(columns={\n",
    "        'CGM (mg / dl)': 'CGM',\n",
    "        'CGM ': 'CGM'\n",
    "    }))\n",
    "\n",
    "    file_names.append(file.split('.')[-2].split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found intervals [60.] minutes apart!\n",
      "Found intervals [30.] minutes apart!\n",
      "Found intervals [30. 45.] minutes apart!\n",
      "Found intervals [45.] minutes apart!\n",
      "Found intervals [30. 30. 30. 30. 30.] minutes apart!\n"
     ]
    }
   ],
   "source": [
    "check = check_15_min_intervals(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shanghai T2DM is in the appropriate time format\n"
     ]
    }
   ],
   "source": [
    "corrected_dataframes_shanghaiT2DM = check_and_fix_15_min_intervals(dataframes,sample_size)\n",
    "check = check_15_min_intervals(corrected_dataframes_shanghaiT2DM)\n",
    "if check == [True for i in range(len(check))]:\n",
    "    print('Shanghai T2DM is in the appropriate time format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minimum value of the timeseries: 64.94862385321106\n",
      "Average mean value of the timeseries: 141.49387217421102\n",
      "Average maximum value of the timeseries: 289.0238532110093\n",
      "Average value from all the timeseries: 140.36590273545335\n",
      "Minimum value from all the timeseries: 39.6\n",
      "Maximum value from all the timeseries: 468.0\n",
      "Average length of the timeseries: 1023.5963302752293\n",
      "Number of timeseries: 109\n"
     ]
    }
   ],
   "source": [
    "print_statistics(corrected_dataframes_shanghaiT2DM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dictionary['ShanghaiT2DM'] = corrected_dataframes_shanghaiT2DM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patient 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records before removing NaN values: 85923\n",
      "Number of records after removing NaN values: 61544\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../../datasets/patient0_raw_data.csv')\n",
    "\n",
    "# Print the number of records before removing NaN values\n",
    "print(f\"Number of records before removing NaN values: {len(df)}\")\n",
    "\n",
    "# Remove rows with NaN values in 'Historic Glucose mg/dL'\n",
    "df_clean = df.dropna(subset=['Historic Glucose mg/dL'])\n",
    "\n",
    "# Print the number of records after removing NaN values\n",
    "print(f\"Number of records after removing NaN values: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Device Timestamp</th>\n",
       "      <th>Historic Glucose mg/dL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24/07/2022 00:12</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24/07/2022 00:27</td>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24/07/2022 00:42</td>\n",
       "      <td>156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24/07/2022 00:57</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24/07/2022 01:12</td>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Device Timestamp  Historic Glucose mg/dL\n",
       "3   24/07/2022 00:12                   162.0\n",
       "4   24/07/2022 00:27                   155.0\n",
       "5   24/07/2022 00:42                   156.0\n",
       "6   24/07/2022 00:57                   163.0\n",
       "11  24/07/2022 01:12                   164.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records before removing duplicates: 61544\n",
      "Number of records after removing duplicates: 61536\n",
      "   Device Timestamp  Historic Glucose mg/dL\n",
      "0  01/01/2023 00:08                   148.0\n",
      "1  01/01/2023 00:23                   166.0\n",
      "2  01/01/2023 00:38                   205.0\n",
      "3  01/01/2023 00:53                   231.0\n",
      "4  01/01/2023 01:08                   248.0\n"
     ]
    }
   ],
   "source": [
    "# Group by 'Device Timestamp' and calculate the mean of 'Historic Glucose mg/dL'\n",
    "df_grouped = df_clean.groupby('Device Timestamp', as_index=False)['Historic Glucose mg/dL'].mean()\n",
    "\n",
    "# Print the number of records before and after removing duplicates\n",
    "print(f\"Number of records before removing duplicates: {len(df_clean)}\")\n",
    "print(f\"Number of records after removing duplicates: {len(df_grouped)}\")\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df_grouped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Device Timestamp' to datetime\n",
    "df_grouped['Device Timestamp'] = pd.to_datetime(df_grouped['Device Timestamp'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "# Sort by timestamp to ensure the time intervals are calculated correctly\n",
    "df_grouped = df_grouped.sort_values(by='Device Timestamp').reset_index(drop=True)\n",
    "\n",
    "# Calculate the intervals between consecutive timestamps\n",
    "intervals = df_grouped['Device Timestamp'].diff().dropna().dt.total_seconds() / 60  # convert to minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_bin_values(data, num_bins):\n",
    "\n",
    "    hist, bin_edges = np.histogram(data, bins=num_bins)\n",
    "\n",
    "    # Print the values in each bin\n",
    "    bins = [[] for _ in range(num_bins)]\n",
    "    for value in data:\n",
    "        for i in range(num_bins):\n",
    "            if bin_edges[i] <= value < bin_edges[i + 1]:\n",
    "                bins[i].append(value)\n",
    "                break\n",
    "    # Include the rightmost edge value in the last bin\n",
    "    bins[-1].extend([value for value in data if value == bin_edges[-1]])\n",
    "\n",
    "    bins_percentage = {}\n",
    "    # Display the bin ranges and the values in each bin\n",
    "    for i in range(num_bins):\n",
    "        bins_percentage[f\"Bin {i + 1} ({bin_edges[i]:.2f} to {bin_edges[i + 1]:.2f})\"] = ((len(bins[i])/len(data)))*100\n",
    "    return bins_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 1000\n",
    "bins_percentage = histogram_bin_values(intervals, num_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bin 2 (12.50 to 24.00)': 99.52384821646217,\n",
       " 'Bin 3 (24.00 to 35.50)': 0.10888112456325669,\n",
       " 'Bin 1 (1.00 to 12.50)': 0.08287966198098642,\n",
       " 'Bin 7 (69.99 to 81.49)': 0.03900219387340538,\n",
       " 'Bin 6 (58.50 to 69.99)': 0.03412691963922971,\n",
       " 'Bin 8 (81.49 to 92.99)': 0.02437637117087836,\n",
       " 'Bin 9 (92.99 to 104.49)': 0.0178760055253108,\n",
       " 'Bin 4 (35.50 to 47.00)': 0.016250914113918907,\n",
       " 'Bin 10 (104.49 to 115.99)': 0.016250914113918907,\n",
       " 'Bin 11 (115.99 to 127.49)': 0.014625822702527016,\n",
       " 'Bin 14 (150.49 to 161.99)': 0.014625822702527016,\n",
       " 'Bin 5 (47.00 to 58.50)': 0.013000731291135124,\n",
       " 'Bin 15 (161.99 to 173.49)': 0.011375639879743236}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(bins_percentage.items(), reverse=True, key=lambda item: item[1]) if v>0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_grouped\n",
    "\n",
    "# Split the time series for large intervals\n",
    "max_interpolation_interval = 40  # in minutes\n",
    "\n",
    "initial_segments = []\n",
    "current_segment = []\n",
    "\n",
    "previous_timestamp = df_clean.loc[0, 'Device Timestamp']\n",
    "current_segment.append(df_clean.loc[0])\n",
    "\n",
    "for i in range(1, len(df_clean)):\n",
    "    current_timestamp = df_clean.loc[i, 'Device Timestamp']\n",
    "    time_diff = (current_timestamp - previous_timestamp).total_seconds() / 60  # difference in minutes\n",
    "    \n",
    "    if time_diff > max_interpolation_interval:\n",
    "        initial_segments.append(pd.DataFrame(current_segment))\n",
    "        current_segment = []\n",
    "    current_segment.append(df_clean.loc[i])\n",
    "    previous_timestamp = current_timestamp\n",
    "\n",
    "# Add the last segment if it's not empty\n",
    "if current_segment:\n",
    "    initial_segments.append(pd.DataFrame(current_segment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segments: 111\n"
     ]
    }
   ],
   "source": [
    "final_segments = []\n",
    "for segment in initial_segments:\n",
    "    if len(segment) > sample_size:\n",
    "        final_segments.append(pd.DataFrame(segment))\n",
    "\n",
    "# Print the number of segments and the number of records in each segment\n",
    "print(f\"Number of segments: {len(final_segments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resample and interpolate a segment\n",
    "def resample_and_interpolate(segment):\n",
    "    interpolated_segment = segment.set_index('Device Timestamp')\n",
    "    interpolated_segment = interpolated_segment.resample('15min').mean()\n",
    "    interpolated_segment['Historic Glucose mg/dL'] = interpolated_segment['Historic Glucose mg/dL'].interpolate(method='linear', limit_direction='both')\n",
    "    interpolated_segment.reset_index(inplace=True)\n",
    "    return interpolated_segment\n",
    "\n",
    "# Apply resampling and interpolation to each segment\n",
    "resampled_segments = [resample_and_interpolate(segment) for segment in final_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_segments = [df.rename(columns={'Device Timestamp':'Date','Historic Glucose mg/dL':'CGM'}) for df in resampled_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory path\n",
    "output_dir = os.path.join('..', '..', 'datasets', 'Patient0')\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the segments to CSV files in the specified directory\n",
    "for i, segment in enumerate(resampled_segments):\n",
    "    segment.to_csv(os.path.join(output_dir, f'resampled_segment_{i+1}.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"../../datasets/Patient0\"\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "csv_files = glob.glob(os.path.join(file_path, \"*.csv\"))\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    if df.isna().any().any():\n",
    "        print(f\"DataFrame {file} has columns with NaN values:\")\n",
    "        print(df.isna().sum())\n",
    "        continue\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d %H:%M:%S')\n",
    "    dataframes.append(df.sort_values(by='Date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = check_15_min_intervals(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient0 Dataset is in the appropriate time format\n",
      "Average minimum value of the timeseries: 66.69369369369369\n",
      "Average mean value of the timeseries: 158.3611250229459\n",
      "Average maximum value of the timeseries: 310.3063063063063\n",
      "Average value from all the timeseries: 160.35261472317276\n",
      "Minimum value from all the timeseries: 40.0\n",
      "Maximum value from all the timeseries: 483.0\n",
      "Average length of the timeseries: 513.3783783783783\n",
      "Number of timeseries: 111\n"
     ]
    }
   ],
   "source": [
    "corrected_dataframes_Patient0 = check_and_fix_15_min_intervals(dataframes,sample_size)\n",
    "\n",
    "check = check_15_min_intervals(corrected_dataframes_Patient0)\n",
    "if check == [True for i in range(len(check))]:\n",
    "    print('Patient0 Dataset is in the appropriate time format')\n",
    "    \n",
    "print_statistics(corrected_dataframes_Patient0)\n",
    "datasets_dictionary['Patient0'] = corrected_dataframes_Patient0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OhioT1DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframes_on_time_difference(dataframes, sample_size, max_diff_minutes=10):\n",
    "    \"\"\"\n",
    "    Splits DataFrames in the list when the time difference between consecutive rows is more than max_diff_minutes.\n",
    "\n",
    "    Parameters:\n",
    "    dataframes (list of pd.DataFrame): List of DataFrames to split.\n",
    "    max_diff_minutes (int): Maximum allowed time difference in minutes between consecutive rows.\n",
    "\n",
    "    Returns:\n",
    "    split_dataframes (list of pd.DataFrame): List of split DataFrames.\n",
    "    \"\"\"\n",
    "    split_dataframes = []\n",
    "    \n",
    "    for df in dataframes:\n",
    "        # Ensure the 'Date' column is sorted\n",
    "        df = df.sort_values(by='Date').reset_index(drop=True)\n",
    "        \n",
    "        # Calculate the time differences in minutes\n",
    "        time_diffs = df['Date'].diff().dt.total_seconds() / 60\n",
    "        \n",
    "        # Initialize the current segment\n",
    "        current_segment = [df.iloc[0]]\n",
    "        \n",
    "        for i in range(1, len(df)):\n",
    "            if time_diffs.iloc[i] > max_diff_minutes:\n",
    "                # If the time difference is greater than max_diff_minutes, finalize the current segment\n",
    "                split_dataframes.append(pd.DataFrame(current_segment))\n",
    "                # Start a new segment\n",
    "                current_segment = [df.iloc[i]]\n",
    "            else:\n",
    "                # Otherwise, add the row to the current segment\n",
    "                current_segment.append(df.iloc[i])\n",
    "        \n",
    "        # Add the last segment\n",
    "        if current_segment:\n",
    "            split_dataframes.append(pd.DataFrame(current_segment))\n",
    "    split_dataframes = [df for df in split_dataframes if len(df) >= sample_size]\n",
    "    return split_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training DataFrames: 12\n",
      "Number of testing DataFrames: 12\n",
      "First training DataFrame:\n",
      "                 Date  CGM\n",
      "0 2027-05-19 11:36:00   76\n",
      "1 2027-05-19 11:41:00   72\n",
      "2 2027-05-19 11:46:00   68\n",
      "3 2027-05-19 11:51:00   65\n",
      "4 2027-05-19 11:56:00   63\n",
      "First testing DataFrame:\n",
      "                 Date  CGM\n",
      "0 2027-07-04 00:02:00  254\n",
      "1 2027-07-04 00:07:00  250\n",
      "2 2027-07-04 00:12:00  249\n",
      "3 2027-07-04 00:17:00  247\n",
      "4 2027-07-04 00:22:00  242\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Define the directory containing the XML files\n",
    "dataset_dir = os.path.join('..', '..', 'datasets', 'OhioT1DM')\n",
    "\n",
    "# Lists to store the DataFrames\n",
    "ohiot1dm_training = []\n",
    "ohiot1dm_testing = []\n",
    "\n",
    "# Function to parse an XML file and convert to a DataFrame\n",
    "def parse_xml_to_df(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    data = []\n",
    "\n",
    "    for event in root.find('glucose_level'):\n",
    "        timestamp = event.get('ts')\n",
    "        value = event.get('value')\n",
    "        df = {'Date': pd.to_datetime(timestamp, format='%d-%m-%Y %H:%M:%S'), 'CGM': int(value)}\n",
    "        df['Date'] = df['Date'].round(freq='min')\n",
    "        data.append(df)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Loop through the files in the directory\n",
    "for filename in os.listdir(dataset_dir):\n",
    "    if filename.endswith('-training.xml'):\n",
    "        df = parse_xml_to_df(os.path.join(dataset_dir, filename))\n",
    "        ohiot1dm_training.append(df)\n",
    "    elif filename.endswith('-testing.xml'):\n",
    "        df = parse_xml_to_df(os.path.join(dataset_dir, filename))\n",
    "        ohiot1dm_testing.append(df)\n",
    "\n",
    "print(f\"Number of training DataFrames: {len(ohiot1dm_training)}\")\n",
    "print(f\"Number of testing DataFrames: {len(ohiot1dm_testing)}\")\n",
    "\n",
    "# Optionally, print the first few rows of the first DataFrame in each list to verify\n",
    "if ohiot1dm_training:\n",
    "    print(f\"First training DataFrame:\\n{ohiot1dm_training[0].head()}\")\n",
    "if ohiot1dm_testing:\n",
    "    print(f\"First testing DataFrame:\\n{ohiot1dm_testing[0].head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training DataFrames after split: 339\n",
      "Number of testing DataFrames after split: 87\n"
     ]
    }
   ],
   "source": [
    "ohiot1dm_training_split = split_dataframes_on_time_difference(ohiot1dm_training, sample_size)\n",
    "ohiot1dm_testing_split = split_dataframes_on_time_difference(ohiot1dm_testing, sample_size)\n",
    "\n",
    "# Example usage:\n",
    "print(f\"Number of training DataFrames after split: {len(ohiot1dm_training_split)}\")\n",
    "print(f\"Number of testing DataFrames after split: {len(ohiot1dm_testing_split)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OhioT1DM training is in the appropriate time format\n",
      "OhioT1DM testing is in the appropriate time format\n"
     ]
    }
   ],
   "source": [
    "corrected_dataframes_ohiot1dm_training = check_and_fix_5_min_intervals(ohiot1dm_training_split,sample_size)\n",
    "check = check_5_min_intervals(corrected_dataframes_ohiot1dm_training)\n",
    "if check == [True for i in range(len(check))]:\n",
    "    print('OhioT1DM training is in the appropriate time format')\n",
    "\n",
    "corrected_dataframes_ohiot1dm_testing = check_and_fix_5_min_intervals(ohiot1dm_testing_split,sample_size)\n",
    "check = check_5_min_intervals(corrected_dataframes_ohiot1dm_testing)\n",
    "if check == [True for i in range(len(check))]:\n",
    "    print('OhioT1DM testing is in the appropriate time format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_15_min_intervals(dataframes,sample_size):\n",
    "    \"\"\"\n",
    "    Retain only the rows where the 'Date' column timestamps are exactly 15 minutes apart.\n",
    "\n",
    "    Parameters:\n",
    "    dataframes (list of pd.DataFrame): List of DataFrames to filter.\n",
    "\n",
    "    Returns:\n",
    "    filtered_dataframes (list of pd.DataFrame): List of filtered DataFrames.\n",
    "    \"\"\"\n",
    "    filtered_dataframes = []\n",
    "\n",
    "    for df in dataframes:\n",
    "        # Ensure the 'Date' column is sorted\n",
    "        df = df.sort_values(by='Date').reset_index(drop=True)\n",
    "        \n",
    "        # Filter rows to keep only those 15 minutes apart\n",
    "        df_filtered = df.iloc[::3].reset_index(drop=True)\n",
    "        \n",
    "        filtered_dataframes.append(df_filtered)\n",
    "    filtered_dataframes = [df for df in filtered_dataframes if len(df) > sample_size]\n",
    "    return filtered_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resample OhioT1DM from 5 minutes to 15 minutes\n",
    "corrected_dataframes_ohiot1dm_training = keep_15_min_intervals(corrected_dataframes_ohiot1dm_training,sample_size)\n",
    "corrected_dataframes_ohiot1dm_testing = keep_15_min_intervals(corrected_dataframes_ohiot1dm_testing,sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minimum value of the timeseries: 62.1796928940168\n",
      "Average mean value of the timeseries: 159.4717722122069\n",
      "Average maximum value of the timeseries: 295.3803706496851\n",
      "Average value from all the timeseries: 159.78201816334987\n",
      "Minimum value from all the timeseries: 40.0\n",
      "Maximum value from all the timeseries: 400.0\n",
      "Average length of the timeseries: 243.59859154929578\n",
      "Number of timeseries: 142\n"
     ]
    }
   ],
   "source": [
    "for df in corrected_dataframes_ohiot1dm_testing:\n",
    "    corrected_dataframes_ohiot1dm_training.append(df)\n",
    "print_statistics(corrected_dataframes_ohiot1dm_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dictionary['OhioT1DM'] = corrected_dataframes_ohiot1dm_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DINAMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of DataFrames: 9\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Define the directory containing the csv files\n",
    "dataset_dir = os.path.join('..', '..', 'datasets', 'Dinamo_T1DM')\n",
    "\n",
    "# Loop through the files in the folder\n",
    "for i in range(1, 10):\n",
    "    file_path = f'{dataset_dir}/glucose_{i}.csv'\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Drop the 'comments' column\n",
    "    df = df.drop(columns=['comments'])\n",
    "    \n",
    "    # Keep only rows where 'type' is 'cgm'\n",
    "    df = df[df['type'] == 'cgm']\n",
    "    \n",
    "    # Drop the 'type' column\n",
    "    df = df.drop(columns=['type'])\n",
    "    \n",
    "    # Create a new 'Date' column by combining 'data' and 'time'\n",
    "    df['Date'] = pd.to_datetime(df['date'] + ' ' + df['time']).dt.round('min')\n",
    "    \n",
    "    # Drop rows with any NaN or empty values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Drop the 'data' and 'time' columns\n",
    "    df = df.drop(columns=['date', 'time'])\n",
    "    \n",
    "    # Rename the 'glucose' column to 'CGM' and multiply the values by 18.018\n",
    "    df['CGM'] = df['glucose'] * 18.018\n",
    "    \n",
    "    # Drop the old 'glucose' column\n",
    "    df = df.drop(columns=['glucose'])\n",
    "    \n",
    "    # Append the cleaned dataframe to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "print(f\"Number of DataFrames: {len(dataframes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found intervals [43205. 38885. 43205. 41765. 43205.] minutes apart!\n",
      "Found intervals [340.] minutes apart!\n",
      "Found intervals [1255.] minutes apart!\n"
     ]
    }
   ],
   "source": [
    "check = check_5_min_intervals(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DinamoT1DM is in the appropriate time format\n"
     ]
    }
   ],
   "source": [
    "corrected_dataframes_Dinamo = check_and_fix_5_min_intervals(dataframes,sample_size)\n",
    "corrected_dataframes_Dinamo = keep_15_min_intervals(corrected_dataframes_Dinamo,sample_size)\n",
    "\n",
    "check = check_15_min_intervals(corrected_dataframes_Dinamo)\n",
    "if check == [True for i in range(len(check))]:\n",
    "    print('DinamoT1DM is in the appropriate time format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minimum value of the timeseries: 57.400200000000005\n",
      "Average mean value of the timeseries: 162.85046053458015\n",
      "Average maximum value of the timeseries: 317.11680000000007\n",
      "Average value from all the timeseries: 164.63681276595747\n",
      "Minimum value from all the timeseries: 39.6396\n",
      "Maximum value from all the timeseries: 399.9996\n",
      "Average length of the timeseries: 302.14285714285717\n",
      "Number of timeseries: 7\n"
     ]
    }
   ],
   "source": [
    "print_statistics(corrected_dataframes_Dinamo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dictionary['DinamoT1DM'] = corrected_dataframes_ohiot1dm_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate training, validation and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examples_estimation(dataset_name, data, split_ratios,context_window,prediction_window):\n",
    "\n",
    "    # Load the datasets\n",
    "    print(f\"Available samples for {dataset_name} datasets...\")\n",
    "    train_ratio,val_ratio,test_ratio = split_ratios\n",
    "    train_examples, val_examples, test_examples = 0, 0, 0\n",
    "\n",
    "    assert(train_ratio+val_ratio+test_ratio==1)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        num_rows = len(data[i])\n",
    "        sample_size = context_window + prediction_window\n",
    "        if num_rows>sample_size:\n",
    "            if num_rows < 3*sample_size + 1:\n",
    "                training_samples = (num_rows - sample_size + 1)\n",
    "                validation_samples = 0\n",
    "                testing_samples = 0\n",
    "            else:\n",
    "                available_samples = (num_rows - 3*sample_size - 1)\n",
    "                training_samples = int(available_samples * train_ratio)\n",
    "                validation_samples = int(available_samples * val_ratio)\n",
    "                testing_samples = int(available_samples * test_ratio)\n",
    "            \n",
    "                if validation_samples == 0:\n",
    "                    training_samples -= 1\n",
    "                    validation_samples += 1\n",
    "                if testing_samples == 0:\n",
    "                    training_samples -= 1\n",
    "                    testing_samples += 1\n",
    "            train_examples += training_samples\n",
    "            val_examples += validation_samples\n",
    "            test_examples += testing_samples\n",
    "\n",
    "    print(f\"Number of training examples: {train_examples}\")\n",
    "    print(f\"Number of validation examples: {val_examples}\")\n",
    "    print(f\"Number of test examples: {test_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dictionary['Dinamo_Shanghai_T1DM'] = datasets_dictionary['DinamoT1DM'] + datasets_dictionary['ShanghaiT1DM']\n",
    "datasets_dictionary['Dinamo_Shanghai_Ohio_T1DM'] = datasets_dictionary['DinamoT1DM'] + datasets_dictionary['ShanghaiT1DM']+ datasets_dictionary['OhioT1DM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available samples for ShanghaiT1DM datasets...\n",
      "Number of training examples: 9436\n",
      "Number of validation examples: 516\n",
      "Number of test examples: 516\n",
      "Available samples for ShanghaiT2DM datasets...\n",
      "Number of training examples: 69327\n",
      "Number of validation examples: 3772\n",
      "Number of test examples: 3772\n",
      "Available samples for Patient0 datasets...\n",
      "Number of training examples: 29748\n",
      "Number of validation examples: 1315\n",
      "Number of test examples: 1315\n",
      "Available samples for OhioT1DM datasets...\n",
      "Number of training examples: 12392\n",
      "Number of validation examples: 208\n",
      "Number of test examples: 208\n",
      "Available samples for DinamoT1DM datasets...\n",
      "Number of training examples: 12392\n",
      "Number of validation examples: 208\n",
      "Number of test examples: 208\n",
      "Available samples for Dinamo_Shanghai_T1DM datasets...\n",
      "Number of training examples: 21828\n",
      "Number of validation examples: 724\n",
      "Number of test examples: 724\n",
      "Available samples for Dinamo_Shanghai_Ohio_T1DM datasets...\n",
      "Number of training examples: 34220\n",
      "Number of validation examples: 932\n",
      "Number of test examples: 932\n"
     ]
    }
   ],
   "source": [
    "for key, value in datasets_dictionary.items():\n",
    "    examples_estimation(key, value, split_ratios,context_length,prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to a file\n",
    "with open('dataset_dictionary.pkl', 'wb') as file:\n",
    "    pickle.dump(datasets_dictionary, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dws_atml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
